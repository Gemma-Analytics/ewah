"""
Modified version of the google analytics to s3 operator
Source of original: https://github.com/airflow-plugins/google_analytics_plugin/blob/master/operators/google_analytics_reporting_to_s3_operator.py
Accessed 5 March 2019
"""
from ewah.ewah_operators.ewah_base_operator import EWAHBaseOperator
from ewah.ewah_utils.airflow_utils import datetime_from_string
from ewah.constants import EWAHConstants as EC

from airflow.hooks.base_hook import BaseHook

import json
import time
from datetime import timedelta
from apiclient.discovery import build
from oauth2client.service_account import ServiceAccountCredentials as SAC


class EWAHGAOperator(EWAHBaseOperator):

    template_fields = ('data_from', 'data_until')

    def __init__(
        self,
        google_conn_id,
        view_id,
        data_from,
        data_until,
        dimensions,
        metrics,
        page_size=10000,
        include_empty_rows=True,
        sampling_level=None,
        chunking_interval=None, # must be timedelta
        reload_data_from=None, # If a new table is added in production, and
        #   it is loading incrementally, where to start loading data? datetime
        reload_data_chunking=None, # must be timedelta
    *args, **kwargs):
        if kwargs.get('update_on_columns'):
            raise Exception('update_on_columns supplied, but the field is ' \
                + 'auto-generated by the operator!')
        if reload_data_from and not (reload_data_chunking or chunking_interval):
            raise Exception('When setting reload_data_from, must also set ' \
                + 'either reload_data_chunking or chunking_interval!')

        dimensions = [
            ('' if dim.startswith('ga:') else 'ga:') + dim
            for dim in dimensions
        ]
        metrics = [
            ('' if metric.startswith('ga:') else 'ga:') + metric
            for metric in metrics
        ]

        kwargs.update({'update_on_columns': [dim[3:] for dim in dimensions]})

        super().__init__(*args, **kwargs)

        #self.google_conn_id = google_conn_id
        self.credentials = BaseHook.get_connection(google_conn_id)
        self.credentials = self.credentials.extra_dejson
        self.view_id = view_id
        self.data_from = data_from
        self.data_until = data_until
        self.sampling_level = sampling_level
        self.dimensions = dimensions
        self.metrics = metrics
        self.page_size = page_size
        self.include_empty_rows = include_empty_rows
        self.chunking_interval = chunking_interval
        self.reload_data_from = reload_data_from
        self.reload_data_chunking = reload_data_chunking or chunking_interval

        self.metricMap = {
            'METRIC_TYPE_UNSPECIFIED': 'varchar(255)',
            'CURRENCY': 'decimal(20,5)',
            'INTEGER': 'int(11)',
            'FLOAT': 'decimal(20,5)',
            'PERCENT': 'decimal(20,5)',
            'TIME': 'time'
        }

        if chunking_interval and not (type(chunking_interval) == timdelta):
            raise Exception('If supplied, chunking_interval must be timedelta!')


        if not self.credentials.get('client_secrets'):
            raise Exception('Google Analytics Credentials misspecified!' \
                + ' Example of a correct specifidation: {0}'.format(
                    json.dumps({"client_secrets":{
                        "type": "service_account",
                        "project_id": "abc-123",
                        "private_key_id": "123456abcder",
                        "private_key": "-----BEGIN PRIVATE KEY-----\nxxx\n-----END PRIVATE KEY-----\n",
                        "client_email": "xyz@abc-123.iam.gserviceaccount.com",
                        "client_id": "123457",
                        "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                        "token_uri": "https://oauth2.googleapis.com/token",
                        "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
                        "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/xyz%40abc-123.iam.gserviceaccount.com"
                    }})
                ))

        if (len(dimensions) + len(metrics)) > 8:
            raise Exception(('Can only fetch up to 8 mentrics & dimensions!' \
                + ' Currently {0} Metrics and {1} Dimensions' \
                + ' (Makes {2} total)!').format(
                    str(len(metrics)),
                    str(len(dimensions)),
                    str(len(dimensions) + len(metrics)),
                ))

        if self.page_size > 10000:
            raise Exception(
                'Please specify a page size equal to or lower than 10000.')


    def execute(self, context):

        if type(self.data_until) == str:
            self.data_until = datetime_from_string(self.data_until)
        if type(self.data_from) == str:
            self.data_from = datetime_from_string(self.data_from)

        if not self.drop_and_replace and not self.test_if_target_table_exists():
            self.chunking_interval = self.reload_data_chunking \
                or self.chunking_interval \
                or (self.data_until - self.data_from)
            self.data_from = self.reload_data_from or context['dag'].start_date
            if type(self.data_from) == str:
                self.data_from = datetime_from_string(self.data_from)
        elif not self.chunking_interval:
            self.chunking_interval = self.data_until - self.data_from

        self.log.info('Connecting to Google...')
        service_object = build(
            'analyticsreporting',
            'v4',
            credentials=SAC.from_json_keyfile_dict(
                self.credentials['client_secrets'],
                ['https://www.googleapis.com/auth/analytics.readonly'],
            )
        )

        i = 0
        while self.data_from < self.data_until:
            i += 1

            since_formatted = self.data_from.strftime('%Y-%m-%d')
            until_formatted = min(
                    self.data_from + self.chunking_interval,
                    self.data_until,
                ).strftime('%Y-%m-%d')
            self.log.info('Chunk {0}: From {1} through {2}'.format(
                str(i),
                since_formatted,
                until_formatted,
            ))

            reportRequest = {
                'viewId': self.view_id,
                'dateRanges': [{
                    'startDate': since_formatted,
                    'endDate': until_formatted,
                }],
                'samplingLevel': self.sampling_level,
                'dimensions': [{'name':d} for d in self.dimensions],
                'metrics': [{'expression':m} for m in self.metrics],
                'pageSize': self.page_size,
                'includeEmptyRows': self.include_empty_rows,
            }

            response = (service_object
                        .reports()
                        .batchGet(body={'reportRequests':[reportRequest]})
                        .execute())

            if response.get('reports'):
                report = response['reports'][0]
                rows = report.get('data', {}).get('rows', [])

                while report.get('nextPageToken'):
                    time.sleep(1)
                    reportRequest.update({'pageToken': report['nextPageToken']})
                    response = (service_object
                                .reports()
                                .batchGet(body={'reportRequests':[reportRequest]})
                                .execute())
                    report = response['reports'][0]
                    rows.extend(report.get('data', {}).get('rows', []))

                if report['data']:
                    report['data']['rows'] = rows
            else:
                report = {}

            columnHeader = report.get('columnHeader', {})
            # Right now all dimensions are hardcoded to varchar(255), will need a
            # map if any non-varchar dimensions are used in the future
            # Unfortunately the API does not send back types for Dimensions like it
            # does for Metrics (yet..)
            dimensionHeaders = [
                {
                    'name': header.replace('ga:', ''),
                    'type': 'varchar(255)',
                }
                for header
                in columnHeader.get('dimensions', [])
            ]
            metricHeaders = [
                {
                    'name': entry.get('name').replace('ga:', ''),
                    'type': self.metricMap.get(entry.get('type'), 'varchar(255)'),
                }
                for entry in
                columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])
            ]

            uploadable_data = []
            rows = report.get('data', {}).get('rows', [])
            for row_counter, row in enumerate(rows):
                root_data_obj = {}
                dimensions = row.get('dimensions', [])
                metrics = row.get('metrics', [])

                for index, dimension in enumerate(dimensions):
                    header = dimensionHeaders[index].get('name') # .lower()
                    root_data_obj[header] = dimension

                for metric in metrics:
                    data = {}
                    data.update(root_data_obj)

                    for index, value in enumerate(metric.get('values', [])):
                        header = metricHeaders[index].get('name') # .lower()
                        data[header] = value

                    # data['viewid'] = self.view_id
                    # data['timestamp'] = self.since

                    uploadable_data += [data]

            self.upload_data(uploadable_data)
            self.data_from += self.chunking_interval
